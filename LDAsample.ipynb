{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1 2 6 0 2 3 1 1 0 0 3', u'1 3 0 1 3 0 0 2 0 0 1', u'1 4 1 0 0 4 9 0 1 2 0', u'2 1 0 3 0 0 5 0 2 3 9', u'3 1 1 9 3 0 2 0 0 1 3', u'4 2 0 3 4 5 1 1 1 4 0', u'2 1 0 3 0 0 5 0 2 2 9', u'1 1 1 9 2 1 2 0 0 1 3', u'4 4 0 3 4 2 1 3 0 0 0', u'2 8 2 0 3 0 2 0 2 7 2', u'1 1 1 9 0 2 2 0 0 3 3', u'4 1 0 0 4 5 1 3 0 1 0']\n",
      "\n",
      "[DenseVector([1.0, 2.0, 6.0, 0.0, 2.0, 3.0, 1.0, 1.0, 0.0, 0.0, 3.0]), DenseVector([1.0, 3.0, 0.0, 1.0, 3.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0]), DenseVector([1.0, 4.0, 1.0, 0.0, 0.0, 4.0, 9.0, 0.0, 1.0, 2.0, 0.0]), DenseVector([2.0, 1.0, 0.0, 3.0, 0.0, 0.0, 5.0, 0.0, 2.0, 3.0, 9.0]), DenseVector([3.0, 1.0, 1.0, 9.0, 3.0, 0.0, 2.0, 0.0, 0.0, 1.0, 3.0]), DenseVector([4.0, 2.0, 0.0, 3.0, 4.0, 5.0, 1.0, 1.0, 1.0, 4.0, 0.0]), DenseVector([2.0, 1.0, 0.0, 3.0, 0.0, 0.0, 5.0, 0.0, 2.0, 2.0, 9.0]), DenseVector([1.0, 1.0, 1.0, 9.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 3.0]), DenseVector([4.0, 4.0, 0.0, 3.0, 4.0, 2.0, 1.0, 3.0, 0.0, 0.0, 0.0]), DenseVector([2.0, 8.0, 2.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 7.0, 2.0]), DenseVector([1.0, 1.0, 1.0, 9.0, 0.0, 2.0, 2.0, 0.0, 0.0, 3.0, 3.0]), DenseVector([4.0, 1.0, 0.0, 0.0, 4.0, 5.0, 1.0, 3.0, 0.0, 1.0, 0.0])]\n",
      "\n",
      "[[0, DenseVector([1.0, 2.0, 6.0, 0.0, 2.0, 3.0, 1.0, 1.0, 0.0, 0.0, 3.0])], [1, DenseVector([1.0, 3.0, 0.0, 1.0, 3.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0])], [2, DenseVector([1.0, 4.0, 1.0, 0.0, 0.0, 4.0, 9.0, 0.0, 1.0, 2.0, 0.0])], [3, DenseVector([2.0, 1.0, 0.0, 3.0, 0.0, 0.0, 5.0, 0.0, 2.0, 3.0, 9.0])], [4, DenseVector([3.0, 1.0, 1.0, 9.0, 3.0, 0.0, 2.0, 0.0, 0.0, 1.0, 3.0])], [5, DenseVector([4.0, 2.0, 0.0, 3.0, 4.0, 5.0, 1.0, 1.0, 1.0, 4.0, 0.0])], [6, DenseVector([2.0, 1.0, 0.0, 3.0, 0.0, 0.0, 5.0, 0.0, 2.0, 2.0, 9.0])], [7, DenseVector([1.0, 1.0, 1.0, 9.0, 2.0, 1.0, 2.0, 0.0, 0.0, 1.0, 3.0])], [8, DenseVector([4.0, 4.0, 0.0, 3.0, 4.0, 2.0, 1.0, 3.0, 0.0, 0.0, 0.0])], [9, DenseVector([2.0, 8.0, 2.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 7.0, 2.0])], [10, DenseVector([1.0, 1.0, 1.0, 9.0, 0.0, 2.0, 2.0, 0.0, 0.0, 3.0, 3.0])], [11, DenseVector([4.0, 1.0, 0.0, 0.0, 4.0, 5.0, 1.0, 3.0, 0.0, 1.0, 0.0])]]\n",
      "\n",
      "Learned topics (as distributions over vocab of 11 words):\n",
      "Topic 0:\n",
      " 4.67228666446\n",
      " 9.80109264214\n",
      " 2.2508576015\n",
      " 4.84776773357\n",
      " 2.53665758857\n",
      " 2.82938714449\n",
      " 19.806916324\n",
      " 0.58515264488\n",
      " 5.42992143905\n",
      " 16.2609696372\n",
      " 20.5706375899\n",
      "Topic 1:\n",
      " 12.4332538515\n",
      " 8.54441929531\n",
      " 1.39497190391\n",
      " 18.347814976\n",
      " 12.0940221668\n",
      " 12.4577620042\n",
      " 4.09233874901\n",
      " 6.52534845163\n",
      " 1.23427371296\n",
      " 4.4508860397\n",
      " 4.36235380353\n",
      "Topic 2:\n",
      " 8.894459484\n",
      " 10.6544880625\n",
      " 8.35417049459\n",
      " 16.8044172905\n",
      " 10.3693202446\n",
      " 6.71285085128\n",
      " 7.10074492694\n",
      " 2.88949890349\n",
      " 1.33580484799\n",
      " 3.2881443231\n",
      " 8.06700860657\n",
      "\n",
      "[[  4.67228666  12.43325385   8.89445948]\n",
      " [  9.80109264   8.5444193   10.65448806]\n",
      " [  2.2508576    1.3949719    8.35417049]\n",
      " [  4.84776773  18.34781498  16.80441729]\n",
      " [  2.53665759  12.09402217  10.36932024]\n",
      " [  2.82938714  12.457762     6.71285085]\n",
      " [ 19.80691632   4.09233875   7.10074493]\n",
      " [  0.58515264   6.52534845   2.8894989 ]\n",
      " [  5.42992144   1.23427371   1.33580485]\n",
      " [ 16.26096964   4.45088604   3.28814432]\n",
      " [ 20.57063759   4.3623538    8.06700861]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Load and parse the data\n",
    "# 這是一份假設11個詞在12篇文章中出現的次數的文件:\n",
    "# 1 2 6 0 2 3 1 1 0 0 3\n",
    "# 1 3 0 1 3 0 0 2 0 0 1\n",
    "# 1 4 1 0 0 4 9 0 1 2 0\n",
    "# 2 1 0 3 0 0 5 0 2 3 9\n",
    "# 3 1 1 9 3 0 2 0 0 1 3\n",
    "# 4 2 0 3 4 5 1 1 1 4 0\n",
    "# 2 1 0 3 0 0 5 0 2 2 9\n",
    "# 1 1 1 9 2 1 2 0 0 1 3\n",
    "# 4 4 0 3 4 2 1 3 0 0 0\n",
    "# 2 8 2 0 3 0 2 0 2 7 2\n",
    "# 1 1 1 9 0 2 2 0 0 3 3\n",
    "# 4 1 0 0 4 5 1 3 0 1 0\n",
    "\n",
    "data = sc.textFile(\"file:/home/cloudera/Desktop/sample_lda_data.txt\")\n",
    "# 讀入txt檔, 每一行當作一個字串\n",
    "print data.collect()\n",
    "print \n",
    "\n",
    "parsedData = data.map(\n",
    "    lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n",
    "print parsedData.collect()\n",
    "print \n",
    "\n",
    "# Index documents with unique IDs\n",
    "corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n",
    "print corpus.collect()\n",
    "print \n",
    "\n",
    "# Cluster the documents into three topics using LDA\n",
    "ldaModel = LDA.train(corpus, k=3) # k=3 分成3個主題\n",
    "\n",
    "# Output topics. Each is a distribution over words (matching word count vectors)\n",
    "print(\"Learned topics (as distributions over vocab of \" + \\\n",
    "      str(ldaModel.vocabSize()) + \" words):\")\n",
    "# 11個關鍵詞在分類出的3個主題中的個別權重,\n",
    "# 從結果可以看出在topic 0中, 第7, 10, 11詞站很大的權重\n",
    "# 在topic 1中, 第1, 4, 5, 6詞站有較高的權重\n",
    "# 在topic 2中, 第2, 4, 5詞的權重偏高\n",
    "\n",
    "topics = ldaModel.topicsMatrix()\n",
    "for topic in range(3):\n",
    "    print(\"Topic \" + str(topic) + \":\")\n",
    "    for word in range(0, ldaModel.vocabSize()):\n",
    "        print(\" \" + str(topics[word][topic]))\n",
    "print \n",
    "print topics\n",
    "\n",
    "# Save and load model   # 存取模型\n",
    "#model.save(sc, \"myModelPath\")\n",
    "#sameModel = LDAModel.load(sc, \"myModelPath\")\n",
    "\n",
    "# 學習總結: \n",
    "# 1.需要一份wordcount的表,橫列為切出的詞次數, 縱列代表每一篇文章\n",
    "# 2.需要另一份切詞的list對照上述的表才能知道是對應哪一個詞 \n",
    "# 參考資料:\n",
    "# http://blog.jobbole.com/86130/\n",
    "# http://spark.apache.org/docs/latest/mllib-clustering.html#latent-dirichlet-allocation-lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lambda x:x+1 \n",
    "#def lambda(x):\n",
    "    #return x+1\n",
    "    \n",
    "#map:將一個函數映射到一個可以列舉的類型中的每一個元素上\n",
    "\n",
    "#Vectors.dense:把list內的元素都變成float64型態\n",
    "#Vectors.parse:把list同層深度的元素變成{key:value,}格式\n",
    "#Vectors.sparse:把成對的資料變成{Key:value,}格式\n",
    "#squared_distance(vector1, vector2):算出兩個vector之間的距離"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LDA in module pyspark.mllib.clustering:\n",
      "\n",
      "class LDA(__builtin__.object)\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0, topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer='em') from __builtin__.type\n",
      " |      Train a LDA model.\n",
      " |      \n",
      " |      :param rdd:                 RDD of data points\n",
      " |      :param k:                   Number of clusters you want\n",
      " |      :param maxIterations:       Number of iterations. Default to 20\n",
      " |      :param docConcentration:    Concentration parameter (commonly named \"alpha\")\n",
      " |          for the prior placed on documents' distributions over topics (\"theta\").\n",
      " |      :param topicConcentration:  Concentration parameter (commonly named \"beta\" or \"eta\")\n",
      " |          for the prior placed on topics' distributions over terms.\n",
      " |      :param seed:                Random Seed\n",
      " |      :param checkpointInterval:  Period (in iterations) between checkpoints.\n",
      " |      :param optimizer:           LDAOptimizer used to perform the actual calculation.\n",
      " |          Currently \"em\", \"online\" are supported. Default to \"em\".\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
